A Confusion Matrix is a performance measurement tool for machine learning classification problems. It summarizes how many predictions were correct and which types of errors were made.

True Positive (TP): You predicted positive and it’s true.

True Negative (TN): You predicted negative and it’s true.

False Positive (FP): You predicted positive but it’s false (Type I error).

False Negative (FN): You predicted negative but it’s false (Type II error).

From this matrix, you can calculate key metrics like Precision, Recall, and the F1-Score to get a deeper understanding of your model's performance beyond simple accuracy.
